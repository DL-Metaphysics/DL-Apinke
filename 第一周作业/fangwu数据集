
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
#输入数据集
X = [[150, 200, 250, 300, 350, 400, 600]]
y = [[6450, 7450, 8450, 9450, 11450, 15450, 18450]]
X = np.array(X).T
y = np.array(y).T

def gradient_descent(X, y, theta, learning_rate = 0.01, iterations = 100): #批量梯度下降
    
    
    m = len(y)
    # learning_rate = 0.01
    # iterations = 100
    
    
    theta_history = np.zeros((iterations, 2))
    for i in range(iterations):
        prediction = np.dot(X, theta)
        
        theta = theta - (1/m) * learning_rate * (X.T.dot((prediction - y)))
        theta_history[i, :] = theta.T
        
        
    return theta, theta_history
def stocashtic_gradient_descent(X, y, theta, learning_rate=0.01, iterations=10): #随机梯度下降
   
    
    m = len(y)
   
    theta_history = np.zeros((iterations, 2))
    for it in range(iterations):
       
        for i in range(m):
            rand_ind = np.random.randint(0, m)
            X_i = X[rand_ind, :].reshape(1, X.shape[1])
            y_i = y[rand_ind, :].reshape(1, 1)
            prediction = np.dot(X_i, theta)
            
            theta -= (1/m) * learning_rate * (X_i.T.dot((prediction - y_i)))
            
       
        
    return theta
# 从1000次迭代开始，学习率为0.01。从高斯分布的θ开始 批量梯度下降theta0 1
lr =0.01
n_iter = 1
theta = np.random.randn(2, 1)
X_b = np.c_[np.ones((len(X), 1)), X]
theta, theta_history = gradient_descent(X_b, y, theta, lr, n_iter)

print('Theta0:          {:0.3f},\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))
#随机梯度下降theta0 1
lr =0.01
n_iter = 5
theta = np.random.randn(2, 1)
X_b = np.c_[np.ones((len(X), 1)), X]
theta = stocashtic_gradient_descent(X_b, y, theta, lr, n_iter)
print('Theta00:          {:0.3f},\nTheta11:          {:0.3f}'.format(theta[0][0],theta[1][0]))
